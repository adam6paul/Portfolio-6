---
title: "Portfolio 6 - Study 2 analyses"
author: "Adam"
date: "5/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The goal of this portfolio is to run the analyses for study 2 of my thesis. 

## Set up

Bringing in the packages I need.

```{r bringing in packages, echo=TRUE, message=FALSE}
library('psych')
library('dplyr')
library('tidyverse')
library('Hmisc')
library('Rcpp')
library('readr')
library('emmeans')
```


#### Bringing in the data

```{r loading data, echo=TRUE, collapse=TRUE, message=FALSE}
study1 <- read_csv("Study2_data_R_cleaned.csv")

# Ensuring the dataframe is a tibble.
as_tibble(study1)

#Taking a look at the table.
View(study1)
```



## Analyses!

Dropping this from my thesis so we know where we're going.

>For each hypothesis, I had originally intended to conduct a multi-level model with group as a random effect, nesting participant responses within their study session. However, initial analyses suggested that multi-level models were unnecessarily complex; session accounted for very little variance across the different outcomes. Therefore, I decided to run a simplified linear regression models with dummy codes to account for studentsâ€™ Introductory Psychology instructor.  That said, the results from both the multi-level models and the models presented here are very similar. I included the following covariates in all models: gender (dummy coded with women as the reference class), race (dummy coded with White as the reference class), number of group members known prior to the study, and number of classmates known prior to the study. When the omnibus test of condition was significant, I then conducted post-hoc tests to compare each of the conditions with each other, using Sidak correction to correct for multiple comparisons.



Put another way, we're doing simplified linear regression models.

Hypotheses:

H1: Students in the prompted disclosure condition will report greater belonging, at both the institutional and class levels, than students in either the prompted discussion or control conditions.

H2: Students in the prompted disclosure condition will report greater subject area motivation than students in either the prompted discussion or control conditions.

H3: Students in the prompted disclosure condition will feel closer to their groupmates than students in either the prompted discussion or control conditions.

H4: Students in the prompted disclosure condition will score higher on the quiz than students in either the prompted discussion or control conditions.



### Preliminary analyses

Repeat from portfolio 5, because I really need this to be correct.
```{r ensuring condition is character vector}
study1$condition <- as.character(study1$condition)

class(study1$condition)
```

>Important note: 0 is control, 1 is prompted discussion, and 2 is prompted disclosure.

### Correlations

The bivariate correlations for the variables of interest.

```{r all conditions bivariate correlations}
correlations <- study1 %>%
        select(class_belong_comp4, school_belong_comp4, motiv_intrins_comp2, motiv_util_comp3)

rcorr(as.matrix(correlations))
    
```

Class and institutional belonging are correlated, and both levels of motivation are correlated with class belonging.

class belonging and school belonging
    r=.41, p<.001
    
class belonging and intrinsic motivation
    r=.52, p<.001
    
class belonging and utility motivation
    r=.44, p<.001
    
Utility motivation and intrinsic motivation
    r=.65, p<.001.
    
    
    
### Disclosure

This is the measure of how much participants though the questions asked them to disclose.


This marks how the code below works:

The first line of code is the creation of the linear model.

First, we give it a name (lm_disclose). Then, we use the lm command to run the model, with the target variable followed by the predictors.
Then, the last line is the dataset that is being used.

Once that's done, we run summary to find out the results.

```{r disclose model}
lm_disclose = lm(manip_check_comp2~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_disclose) #Review the results

```

We found that condition was a significant predictor, as was Asian, and group members known prior. 

Asian                 B=.524, SE=.251, p=.039

Group members known   B=.233, SE=.115, p=.045

Specifically, we see that the experimental condition is significantly different. But we want to look at the pairwise comparisons to look at how it differs from the other conditions.


```{r post-hoc disclosure}
emmeans(lm_disclose, pairwise ~ condition, adjust= "holm")

```

We see that it is significantly different from both the control and prompted discussion condition.



### Self-disclosure

Up next is how much participants reported actually disclosing.

```{r self disclose model}
lm_self_disclose = lm(self_disclosure_comp3~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_self_disclose) #Review the results

```

Once again, the prompted disclosure condition is significantly different, but none of the covariates are. So, let's turn to the pairwise comparison.


```{r post-hoc self-disclosure}
emmeans(lm_self_disclose, pairwise ~ condition, adjust= "holm")

```

Students reported actually disclosing more in the prompted disclosure condition than either of the other two conditions:
 
Control:              B= -1.172, SE= 0.139, p<.001

Prompted discussion:  B= -1.250, SE= 0.138, p<.001



### Group disclosure

```{r group disclose model}
lm_group_disclose = lm(group_disclosure_comp3~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_group_disclose)

```

There is a significant difference of condition, so let's look at the post-hoc tests.


```{r post-hoc group disclosure}
emmeans(lm_group_disclose, pairwise ~ condition, adjust= "holm")

```

Students reported that their groups disclosed more to them in the prompted disclosure condition than either of the other two conditions:

Control:              B= -1.432, SE= 0.130, p<.001

Prompted discussion:  B= -1.446, SE= 0.129, p<.001



### Group talking

```{r group talking model}
lm_group_talk = lm(group_talk~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_group_talk)

```

Only group members known prior is significant, suggesting that participants are talking equally across all the groups. Because of this, we don't need the post-hoc test. But I'm running it to get the emm and coefficients, since that's what we reported in the thesis.


```{r post-hoc group talking}
emmeans(lm_group_talk, pairwise ~ condition, adjust= "holm")

```



## Primary analyses

### Closeness

```{r closeness model}
lm_closeness = lm(close_comp3~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_closeness)

```

The intercept is significant, so I'd like to look more closely. Additionally, being Black predicts greater closeness, as does one of our sections. The n are not very large for that, so it is probably an artifact of the data.

Black         Est=.557, SE=.208, p=.008


```{r post-hoc closeness}
emmeans(lm_closeness, pairwise ~ condition, adjust= "holm")

```

Students reported feeling closer to their groupmates in the prompted disclosure condition than in the prompted discussion condition. However, the difference was not significant between either and the control.

Prompted discussion (B= -0.318, SE= 0.110, p=.013)



## Belonging


### School belonging

```{r school belonging model}
lm_school_belong = lm(school_belong_comp4~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_school_belong)

```


School belonging is not predicted by condition, but it is predicted negatively by being Asian or Black, and positively by being multiracial (though this is a small n), and finally, classmates known prior is a positive predictor.

Asian             Est= -1.329, SE=.340, p<.001

Black             Est= -1.267, SE=.458, p=.007 

Multiracial       Est= 0.824,  SE=.396, p=.039 

Classmates known  Est= 0.232,  SE=.073, p=.002


```{r post-hoc institutional belonging}
emmeans(lm_school_belong, pairwise ~ condition, adjust= "holm")

```



### Class belonging

```{r class belonging model}
lm_class_belong = lm(class_belong_comp4~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_class_belong)

```


Prompted discussion is a significant predictor, as is classmates known prior. Asian is marginal.

class_know_prior       Est=.265, SE=.069, p<.001

Asian                  Est=-.618, SE=.320, p=.056


```{r post-hoc class belonging}
emmeans(lm_class_belong, pairwise ~ condition, adjust= "holm")

```

Prompted discussion is significantly different from both other conditions.

Prompted discussion to:

Control                B=-.594, SE=.241, p=.034

Prompted disclosure    B=-.584, SE=0.227 p=.034



## Subject Area motivation

### Intrinsic motivation

```{r intrinsic motivation model}
lm_motiv_intrins = lm(motiv_intrins_comp2~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_motiv_intrins)

```

Intercept is significant, as is gender, such that men report greater intrinsic motivation. Classmates known prior is once again a significant positive predictor.

Gender                   Est=-.410, SE=.143, p=.005

Classmates known         Est=.131,  SE=.051, p=.012

Once again, only want the emmeans since that's what we reported.


```{r post-hoc intrinsic motivation}
emmeans(lm_motiv_intrins, pairwise ~ condition, adjust= "holm")

```

There was no effect of condition on intrinsic motivation.






### Utility motivation

```{r utility motivation model}
lm_motiv_util = lm(motiv_util_comp3~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_motiv_util)

```

Prompted discussion is a significant predictor, as is being female, being black, and classmates known prior. Two classes are significant predictors of utility motivation (go those professors)


Gender		Est=-.351, SE=.131, p=.009

Black			Est=.597,  SE=.295, p=.045

Classmates known	Est=.106,  SE=.047, p=.026


```{r post-hoc utility motivation}
emmeans(lm_motiv_util, pairwise ~ condition, adjust= "holm")

```

We found that there was a significant difference between the control and the prompted discussion, such that the prompted discussion was nearly half a point higher on utility motivation than the control.

Control to prompted discussion  
B=-0.448 SE=.165, p=.022



### Quiz performance

```{r quiz score model}
lm_quiz_average = lm(quiz_average~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_quiz_average)

```

The overall model is significant, but the only predictor is Dr. Cameron's class. There is not a significant difference on the scores across conditions.

```{r post-hoc quiz score}
emmeans(lm_quiz_average, pairwise ~ condition, adjust= "holm")

```



## Exploratory analyses


### Thinking deeper

```{r thinking deeper model}
lm_manip_help_think = lm(manip_help_think~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_manip_help_think)

```

None of the conditions are significant, though Hispanic did come out as significant.

Hispanic        Est=.662, SE=.314, p=.037


```{r thinking deeper score}
emmeans(lm_manip_help_think, pairwise ~ condition, adjust= "holm")

```



### Similarity to class

```{r similarity model}
lm_class_similar = lm(class_similar_comp2~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_class_similar)

```

Prompted disclosure is significantly different, as are Greene and Verbecke once more. These two have low n's, so we should be cautious in considering these (and were in my thesis, so I'm going to focus on what we did there).


```{r similarity score}
emmeans(lm_class_similar, pairwise ~ condition, adjust= "holm")

```

The prompted disclosure condition is significantly different than both conditions.

Control                 B=.556, SE=.194, p=.001

Prompted discussion     B=.856, SE=.190, p<.001



#### Task enjoyment

```{r enjoyment model}
lm_group_enjoy = lm(group_engagement_comp3~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_group_enjoy)

```

Asian students reported enjoying the task more, and the overall model was significant.

Asian        Est=.581, SE=.231, p=.013


```{r enjoyment score}
emmeans(lm_group_enjoy, pairwise ~ condition, adjust= "holm")

```

Prompted discussion is significantly different from disclosure, but the control is not significantly different from either.

Prompted discussion     B=-.446 SE=0.164, p=.023


## Wrapping up

That finishes up all of the analyses! But there's a lot of information here, that would be good to make sense of. Portfolio 7 will make graphs for all of the variables of interest, and then portfolio 8 will put all the information together in a presentable way.