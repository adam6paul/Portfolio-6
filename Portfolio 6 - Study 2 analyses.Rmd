---
title: "Portfolio 6 - Study 2 analyses"
author: "Adam"
date: "5/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The goal of this portfolio is to run the analyses for study 2 of my thesis. 

## Set up

Bringing in the packages I need.

```{r bringing in packages, echo=TRUE, message=FALSE}
library('psych')
library('dplyr')
library('tidyverse')
library('Hmisc')
library('Rcpp')
library('readr')
library('emmeans')
```


#### Bringing in the data

```{r loading data, echo=TRUE, collapse=TRUE, message=FALSE}
study1 <- read_csv("Study2_data_R_cleaned.csv")

# Ensuring the dataframe is a tibble.
as_tibble(study1)

#Taking a look at the table.
View(study1)
```



### Analyses!

Dropping this from my thesis so we know where we're going.

>For each hypothesis, I had originally intended to conduct a multi-level model with group as a random effect, nesting participant responses within their study session. However, initial analyses suggested that multi-level models were unnecessarily complex; session accounted for very little variance across the different outcomes. Therefore, I decided to run a simplified linear regression models with dummy codes to account for studentsâ€™ Introductory Psychology instructor.  That said, the results from both the multi-level models and the models presented here are very similar. I included the following covariates in all models: gender (dummy coded with women as the reference class), race (dummy coded with White as the reference class), number of group members known prior to the study, and number of classmates known prior to the study. When the omnibus test of condition was significant, I then conducted post-hoc tests to compare each of the conditions with each other, using Sidak correction to correct for multiple comparisons.



Put another way, we're doing simplified linear regression models.

Hypotheses:

H1: Students in the prompted disclosure condition will report greater belonging, at both the institutional and class levels, than students in either the prompted discussion or control conditions.

H2: Students in the prompted disclosure condition will report greater subject area motivation than students in either the prompted discussion or control conditions.

H3: Students in the prompted disclosure condition will feel closer to their groupmates than students in either the prompted discussion or control conditions.

H4: Students in the prompted disclosure condition will score higher on the quiz than students in either the prompted discussion or control conditions.



### Preliminary analyses

Repeat from portfolio 5, because I really need this to be correct.
```{r ensuring condition is character vector}
study1$condition <- as.character(study1$condition)

class(study1$condition)
```

>Important note: 0 is control, 1 is prompted discussion, and 2 is prompted disclosure.

#### Correlations

The bivariate correlations for the variables of interest.

```{r all conditions bivariate correlations}
correlations <- study1 %>%
        select(class_belong_comp4, school_belong_comp4, motiv_intrins_comp2, motiv_util_comp3)

rcorr(as.matrix(correlations))
    
```

Class and institutional belonging are correlated, and both levels of motivation are correlated with class belonging.
class belonging and school belonging
    r=.41, p<.001
class belonging and intrinsic motivation
    r=.52, p<.001
class belonging and utility motivation
    r=.44, p<.001
Utility motivation and intrinsic motivation
    r=.65, p<.001.
    
#### Disclosure

This is the measure of how much participants though the questions asked them to disclose.

```{r disclose model}
lm_disclose = lm(manip_check_comp2~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_disclose) #Review the results

```

The first line of code is the creation of the linear model.

First, we give it a name (lm_disclose). Then, we use the lm command to run the model, with the target variable followed by the predictors.
Then, the last line is the dataset that is being used.

Once that's done, we run summary to find out the results.

We found that condition was a significant predictor, as was asian, and group members known prior. 

Specifically, we see that the experimental condition is significantly different. But we want to look at the pairwise comparisons to look at how it differs from the other conditions.

```{r post-hoc disclosure}
emmeans(lm_disclose, pairwise ~ condition, adjust= "holm")

```

We see that it is significantly different from both the control and prompted discussion condition.


#### Self-disclosure

Up next is how much participants reported actually disclosing.

```{r self disclose model}
lm_self_disclose = lm(self_disclosure_comp3~ condition + gender_male_dummy + race_asian + race_black + 
    race_multiracial + race_hispanic + section_greene + section_verbecke + section_cameron + know_prior_study + 
    class_know_prior,
                     data = study1)


summary(lm_self_disclose) #Review the results

```

Once again, the prompted disclosure condition is significantly different, but none of the covariates are. So, let's turn to the pairwise comparison.


```{r post-hoc disclosure}
emmeans(lm_self_disclose, pairwise ~ condition, adjust= "holm")

```

Turns out it significantly differs from both conditions on levels of disclosure, which means we successfully manipulated self disclosure.



#### Group disclosure





## Wrapping up

